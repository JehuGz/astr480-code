{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c033e855-7290-4b37-b034-386b89c78816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import astroscrappy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import batman\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.stats import sigma_clip, mad_std\n",
    "from astropy.time import Time\n",
    "from astropy.table import QTable, vstack, hstack\n",
    "from astropy.modeling import models, fitting\n",
    "\n",
    "from photutils.detection import DAOStarFinder\n",
    "from photutils.aperture import CircularAperture, CircularAnnulus, aperture_photometry\n",
    "\n",
    "from datetime import datetime, UTC, timezone\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a543ae-99f3-4881-a211-e78787f2cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_median_bias(bias_list = [\n",
    "    'TOI_4463_Data/Bias_BIN1_20250511_074228.fits',\n",
    "    'TOI_4463_Data/Bias_BIN1_20250511_074243.fits',\n",
    "    'TOI_4463_Data/Bias_BIN1_20250511_074255.fits',\n",
    "    'TOI_4463_Data/Bias_BIN1_20250511_074307.fits',\n",
    "    'TOI_4463_Data/Bias_BIN1_20250511_074318.fits',\n",
    "    'TOI_4463_Data/Bias_BIN1_20250511_074330.fits',\n",
    "    'TOI_4463_Data/Bias_BIN1_20250511_074343.fits'\n",
    "], median_bias_filename = 'median_bias_file.fits'):\n",
    "\n",
    "    # Creating empty list for future trimmed biases.\n",
    "    trimmed_biases = []\n",
    "\n",
    "    # Calling each bias and trimming to 1024x1024, for less memory use.\n",
    "    for fname in bias_list:\n",
    "        with fits.open(fname, memmap=False) as hdul:\n",
    "                data = hdul[0].data\n",
    "                trimmed = data[1536:2560, 1536:2560]  # Center cut of 1024x1024\n",
    "                trimmed_biases.append(trimmed)\n",
    "\n",
    "    # Stacking trimmed biases in a 3D numpy array.\n",
    "    bias_stack = np.stack(trimmed_biases, axis=0)\n",
    "\n",
    "    # Sigma clipping is applied with the 3-sigma along axis=0.\n",
    "    clipped = sigma_clip(bias_stack, sigma=3, axis=0, masked=True)\n",
    "\n",
    "    # Computing bias medians while purposely ignoring masked values.\n",
    "    median_bias = np.ma.median(clipped, axis=0).filled(np.nan)  # Masked pixels are filled with 'nan'\n",
    "\n",
    "    # Defining FITs header with more information.\n",
    "    header = fits.Header()\n",
    "\n",
    "    # Included date, frame number, clipping, sigma clipping threshold, and trimmed region information.\n",
    "    header['DATE'] = datetime.now(UTC).isoformat(), 'File created. UTC'\n",
    "    header['NFRAMES'] = len(bias_list), 'Number of bias frames combined'\n",
    "    header['CLIP'] = 'Sigma-clipping', 'Method of Combination'\n",
    "    header['SIGMA'] = 3.0, 'Threshold of Sigma-clipping'\n",
    "    header['TRIMMED'] = '[1536:2560,1536:2560]', 'Trimmed region of input frames'\n",
    "\n",
    "    # Creating FITs file from the resulting median bias frame. \n",
    "    primary = fits.PrimaryHDU(data=median_bias, header=header)\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(median_bias_filename, overwrite=True)\n",
    "\n",
    "    return median_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a7acab-a351-497f-9b7b-ceb24d48948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_median_dark(dark_list = [\n",
    "    'TOI_4463_Data/Dark_BIN1_20250511_074356.fits',\n",
    "    'TOI_4463_Data/Dark_BIN1_20250511_074459.fits',\n",
    "    'TOI_4463_Data/Dark_BIN1_20250511_074603.fits',\n",
    "    'TOI_4463_Data/Dark_BIN1_20250511_074707.fits',\n",
    "    'TOI_4463_Data/Dark_BIN1_20250511_074812.fits'\n",
    "], bias_filename = 'median_bias_file.fits', median_dark_filename = 'median_dark_file.fits'):\n",
    "\n",
    "    # Empty list for future trimmed darks.\n",
    "    trimmed_darks = []\n",
    "\n",
    "    # Loading in the bias frame.\n",
    "    with fits.open(bias_filename, memmap=False) as hdul_bias:\n",
    "        bias_frame = hdul_bias[0].data\n",
    "        bias_trimmed = bias_frame  # Bias frame is already trimmed, set 'bias_trimmed' to 'bias_frame'\n",
    "\n",
    "    # Processing each of the dark files.\n",
    "    for fname in dark_list:\n",
    "         with fits.open(fname, memmap=False) as hdul:\n",
    "            data = hdul[0].data\n",
    "            header = hdul[0].header\n",
    "\n",
    "    trimmed = data[1536:2560, 1536:2560]  # Trimming the dark frames.\n",
    "    bias_corrected = trimmed - bias_trimmed  # Subtracting biases from dark frames.\n",
    "\n",
    "    # Getting the exposure time 'EXPTIME' column. \n",
    "    try:\n",
    "        exptime = header['EXPTIME']\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Could not find exposure time in header of {fname}\")\n",
    "\n",
    "    # Finding dark current by having corrected bias / exposure time. \n",
    "    dark_current = bias_corrected / exptime\n",
    "\n",
    "    trimmed_darks.append(dark_current) # Appending dark currents to trimmed darks. \n",
    "\n",
    "    # Stacking and performing sigma-clipping on dark frames. \n",
    "    dark_stack = np.stack(trimmed_darks, axis=0)\n",
    "    clipped = sigma_clip(dark_stack, sigma=3, axis=0, masked=True)\n",
    "    median_dark = np.ma.median(clipped, axis=0).filled(np.nan)\n",
    "\n",
    "    # Creating FITs header with information.\n",
    "    header = fits.Header()\n",
    "    header['DATE'] = datetime.now(UTC).isoformat(), 'Date of dark frame creation (UTC)'\n",
    "    header['NFRAMES'] = len(dark_list), 'Number of dark frames combined'\n",
    "    header['CLIP'] = 'Sigma-clipping', 'Combination method'\n",
    "    header['SIGMA'] = 3.0, 'Sigma clipping threshold'\n",
    "    header['TRIMMED'] = '[1536:2560,1536:2560]', 'Trimmed region of input frames'\n",
    "    header['UNITS'] = 'ADU/sec', 'Units are electrons or counts per second'\n",
    "\n",
    "    # Save to a FITs file. \n",
    "    primary = fits.PrimaryHDU(data=median_dark, header=header)\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(median_dark_filename, overwrite=True)\n",
    "\n",
    "    return median_dark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d8e80b-7fbc-428e-be3e-60344477e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_median_flat(flat_list = [\n",
    "    'TOI_4463_Data/domeflat_g_001-2-2.fits',\n",
    "    'TOI_4463_Data/domeflat_g_001-2.fits',\n",
    "    'TOI_4463_Data/domeflat_g_002-2.fits',\n",
    "    'TOI_4463_Data/domeflat_g_002.fits',\n",
    "    'TOI_4463_Data/domeflat_g_003-2.fits',\n",
    "    'TOI_4463_Data/domeflat_g_003.fits',\n",
    "    'TOI_4463_Data/domeflat_g_004-2.fits',\n",
    "    'TOI_4463_Data/domeflat_g_004.fits',\n",
    "    'TOI_4463_Data/domeflat_g_005-2.fits',\n",
    "    'TOI_4463_Data/domeflat_g_005.fits',\n",
    "    'TOI_4463_Data/domeflat_g_006-2.fits',\n",
    "    'TOI_4463_Data/domeflat_g_006.fits',\n",
    "    'TOI_4463_Data/domeflat_g_007-2.fits',\n",
    "    'TOI_4463_Data/domeflat_g_007.fits'\n",
    "],\n",
    "    bias_filename = 'median_bias_file.fits',\n",
    "    median_flat_filename = 'median_flat_file.fits',\n",
    "    dark_filename=None,\n",
    "):\n",
    "\n",
    "    # Empty list for future trimmed flats.\n",
    "    trimmed_flats = []\n",
    "\n",
    "    # Re-calling bias frame.\n",
    "    with fits.open(bias_filename, memmap=False) as hdul_bias:\n",
    "        bias_frame = hdul_bias[0].data\n",
    "        bias_trimmed = bias_frame # Trimmed bias.\n",
    "\n",
    "    # Calling dark frame.\n",
    "    if dark_filename is not None:\n",
    "        with fits.open(dark_filename, memmap=False) as hdul_dark:\n",
    "            dark_frame = hdul_dark[0].data\n",
    "            dark_trimmed = dark_frame # Trimmed and normalized to ADU/sec\n",
    "\n",
    "    # Processing each of the flat files.\n",
    "    for fname in flat_list:\n",
    "        with fits.open(fname, memmap=False) as hdul:\n",
    "            data = hdul[0].data\n",
    "            header = hdul[0].header\n",
    "\n",
    "    trimmed = data[1536:2560, 1536:2560] # Trimming flat files.\n",
    "\n",
    "    corrected = trimmed - bias_trimmed # Subtracting bias from frames.\n",
    "\n",
    "    # Subtracting dark frame.\n",
    "    if dark_filename is not None:\n",
    "        try:\n",
    "            exptime = header['EXPTIME']\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Exposure time not found in header of {fname}\")\n",
    "        corrected -= dark_trimmed * exptime\n",
    "\n",
    "    trimmed_flats.append(corrected)\n",
    "\n",
    "    # Stacking the flats and sigma clipping them. \n",
    "    flat_stack = np.stack(trimmed_flats, axis=0)\n",
    "    clipped = sigma_clip(flat_stack, sigma=3, axis=0, masked=True)\n",
    "    median_flat = np.ma.median(clipped, axis=0).filled(np.nan)\n",
    "\n",
    "    # Normalizing with the median value while ignoring nans.\n",
    "    norm_median_flat = median_flat / np.nanmedian(median_flat)\n",
    "\n",
    "    # Creating a more detailed FITS header with information on data, number of frames, etc. \n",
    "    header = fits.Header()\n",
    "    header['DATE'] = datetime.now(UTC).isoformat(), 'Date of flat field creation (UTC)'\n",
    "    header['NFRAMES'] = len(flat_list), 'Number of flat frames combined'\n",
    "    header['CLIP'] = 'Sigma-clipping', 'Combination method'\n",
    "    header['SIGMA'] = 3.0, 'Sigma clipping threshold'\n",
    "    header['TRIMMED'] = '[1536:2560,1536:2560]', 'Trimmed region of input frames'\n",
    "    header['NORMALIZ'] = 'By median', 'Flat normalized to median'\n",
    "    if dark_filename:\n",
    "        header['DARKSUB'] = True, 'Dark subtraction applied'\n",
    "    else:\n",
    "        header['DARKSUB'] = False, 'No dark subtraction applied'\n",
    "\n",
    "    # Saving to a FITS file. \n",
    "    primary = fits.PrimaryHDU(data=norm_median_flat, header=header)\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(median_flat_filename, overwrite=True)\n",
    "\n",
    "    return median_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19eb2a9f-1bfb-4ed2-ac4d-a5358919e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_science_frame(\n",
    "    science_filename = 'TOI_4463_Data/TOI4463_g_20250511_075808.fits',\n",
    "    median_bias_filename = 'median_bias_file.fits',\n",
    "    median_flat_filename = 'median_flat_file.fits',\n",
    "    median_dark_filename = 'median_dark_file.fits',\n",
    "    reduced_science_filename=\"reduced_science.fits\",\n",
    "    remove_cosmic_rays=True,\n",
    "):\n",
    "\n",
    "    # Reading in the science frame. \n",
    "    with fits.open(science_filename) as hdul:\n",
    "        science_data = hdul[0].data.astype(float)\n",
    "        science_header = hdul[0].header\n",
    "\n",
    "    # Reading in bias frame. \n",
    "    with fits.open(median_bias_filename) as hdul:\n",
    "        bias_data = hdul[0].data.astype(float)\n",
    "\n",
    "    # Reading in median dark frame\n",
    "    with fits.open(median_dark_filename) as hdul:\n",
    "        dark_data = hdul[0].data.astype(float)\n",
    "\n",
    "    # Reading in median normalized flat frame. \n",
    "    with fits.open(median_flat_filename) as hdul:\n",
    "        flat_data = hdul[0].data.astype(float)\n",
    "\n",
    "    # Getting exposure time from science file header. \n",
    "    exptime = science_header.get('EXPTIME')\n",
    "    if exptime is None:\n",
    "        raise ValueError(f\"EXPTIME keyword not found in {science_filename} header.\")\n",
    "\n",
    "    # Need to trim science data to 1024x1024 to match the trimmed bias, dark, and flat files.\n",
    "    if science_data.shape == (1024, 1024):\n",
    "        science_trimmed = science_data\n",
    "    else:\n",
    "        science_trimmed = science_data[1536:2560, 1536:2560]\n",
    "\n",
    "    # Subtracting the bias data from the now trimmed science file.\n",
    "    data_sub_bias = science_trimmed - bias_data\n",
    "    data_sub_dark = data_sub_bias - (dark_data * exptime)\n",
    "\n",
    "    # Applying flat field correction.\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        reduced_data = np.true_divide(data_sub_dark, flat_data)\n",
    "        reduced_data[~np.isfinite(reduced_data)] = 0  # The inf and nan are set to zero.\n",
    "\n",
    "    # Removing cosmic rays when needed. \n",
    "    if remove_cosmic_rays:\n",
    "        \n",
    "        cr_mask, cleaned_data = astroscrappy.detect_cosmics(reduced_data)\n",
    "        reduced_data = cleaned_data # Returning cleaned and masked frame.\n",
    "\n",
    "    # More detailed header with metadata.\n",
    "    science_header['HISTORY'] = 'Reduced with custom pipeline'\n",
    "    science_header['HISTORY'] = f'Bias, dark, flat corrections applied'\n",
    "    science_header['HISTORY'] = f'Cosmic rays removed: {remove_cosmic_rays}'\n",
    "    science_header['REDUCED'] = Time.now().isot\n",
    "    science_header['REDDATE'] = datetime.now(timezone.utc).isoformat(timespec='seconds')\n",
    "    science_header['REDUSER'] = 'Jehu Gonzalez-Zarate'\n",
    "    science_header['BIASFILE'] = median_bias_filename\n",
    "    science_header['DARKFILE'] = median_dark_filename\n",
    "    science_header['FLATFILE'] = median_flat_filename\n",
    "    science_header['COMMENT'] = 'Reduced science frame for photometry'\n",
    "    science_header['COMMENT'] = 'Trimmed to center 1024x1024 region'\n",
    "\n",
    "    # Saving reduced science frame as FITS file.\n",
    "    hdu = fits.PrimaryHDU(data=reduced_data, header=science_header)\n",
    "    hdu.writeto(reduced_science_filename, overwrite=True)\n",
    "\n",
    "    return reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6556bdfe-ee1c-49a8-b34c-fa2f120030e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_reduce_all_science_frames(\n",
    "    science_folder='TOI_4463_Data/',\n",
    "    output_folder='reduced_science',\n",
    "    median_bias='median_bias_file.fits',\n",
    "    median_dark='median_dark_file.fits',\n",
    "    median_flat='median_flat_file.fits'\n",
    "):\n",
    "    # Makes sure to create an output folder.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Sorts through all the science images from TOI_4463_Data folder\n",
    "    science_files = sorted(glob(os.path.join(science_folder, 'TOI4463_g_*.fits')))\n",
    "\n",
    "    print(f\"Found {len(science_files)} science frames.\")\n",
    "\n",
    "    for sci_file in science_files:\n",
    "        filename = os.path.basename(sci_file)\n",
    "        reduced_name = os.path.join(output_folder, f\"reduced_{filename}\")\n",
    "\n",
    "        print(f\"Reducing {filename}...\")\n",
    "\n",
    "        try:\n",
    "            reduce_science_frame(\n",
    "                science_filename=sci_file,\n",
    "                median_bias_filename=median_bias,\n",
    "                median_dark_filename=median_dark,\n",
    "                median_flat_filename=median_flat,\n",
    "                reduced_science_filename=reduced_name,\n",
    "                remove_cosmic_rays=True\n",
    "            )\n",
    "            print(f\"Saved reduced frame to {reduced_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reducing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a2f917-203d-44ec-a277-7c2998a604a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 science frames.\n"
     ]
    }
   ],
   "source": [
    "# Applying reduction on all the science TOI 4463 frames instead of just one. \n",
    "batch_reduce_all_science_frames(\n",
    "    science_folder='TOI_4463_Data',\n",
    "    output_folder='TOI_4463_Data/reduced_science',\n",
    "    median_bias='TOI_4463_Data/median_bias_file-2.fits',\n",
    "    median_dark='TOI_4463_Data/median_dark_file-2.fits',\n",
    "    median_flat='TOI_4463_Data/median_flat_file-2.fits'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4286fbc-aec4-4d4a-913a-7e1d6813adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_aperture_photometry(\n",
    "    image='reduced_science.fits',\n",
    "    positions=[(509.651, 511.326), (363, 345), (832.334, 911.131), (516, 68), (514, 786), (307.08, 760.36)],\n",
    "    radii=[8.32, 9.32, 10.32],\n",
    "    sky_annulus_width=8.0,\n",
    "):\n",
    "    \n",
    "    # Reading data from image\n",
    "    with fits.open(image) as hdul:\n",
    "        data = hdul[0].data\n",
    "\n",
    "    # Empty list for results later on.\n",
    "    all_results = []\n",
    "\n",
    "    for radius in radii:\n",
    "        sky_radius_in = radius + 1.5  # Sky annulus just outside aperture.\n",
    "\n",
    "        aperture = CircularAperture(positions, r=radius)\n",
    "        annulus = CircularAnnulus(positions, r_in=sky_radius_in, r_out=sky_radius_in + sky_annulus_width)\n",
    "\n",
    "        # Background medians are calculated.\n",
    "        annulus_masks = annulus.to_mask(method='center')\n",
    "        bkg_median = []\n",
    "        for mask in annulus_masks:\n",
    "            annulus_data = mask.multiply(data)\n",
    "            annulus_data_1d = annulus_data[mask.data > 0]\n",
    "            median_val = np.median(annulus_data_1d)\n",
    "            bkg_median.append(median_val)\n",
    "        bkg_median = np.array(bkg_median)\n",
    "\n",
    "        phot_table = aperture_photometry(data, aperture)\n",
    "        phot_table.meta.clear() # Clearing meta data.\n",
    "        aperture_area = aperture.area\n",
    "\n",
    "    phot_table['bkg'] = bkg_median\n",
    "    phot_table['bkg_sum'] = bkg_median * aperture_area\n",
    "    phot_table['net_flux'] = phot_table['aperture_sum'] - phot_table['bkg_sum']\n",
    "    phot_table['aperture_radius'] = radius\n",
    "\n",
    "    all_results.append(phot_table)\n",
    "\n",
    "    # Vertically combining tables, with additional column for radius. \n",
    "    from astropy.table import vstack\n",
    "    final_table = vstack(all_results)\n",
    "\n",
    "    return final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "453ff101-bc2d-4283-9df5-6687881f4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_folder='TOI_4463_Data/reduced_science/'\n",
    "\n",
    "# Sorting through science files.\n",
    "science_files = sorted(glob(os.path.join(science_folder, 'reduced_TOI4463_g_*.fits')))\n",
    "all_photometry = [] # Empty list for photometry.\n",
    "\n",
    "# Looping over each file and applying photometry.\n",
    "for file in science_files:\n",
    "    print(f\"Processing: {file}\")\n",
    "    \n",
    "    phot_table = do_aperture_photometry(image=file)\n",
    "\n",
    "    # Adding filenames.\n",
    "    phot_table['filename'] = os.path.basename(file)\n",
    "\n",
    "    # Extracting time from header for future plotting.\n",
    "    with fits.open(file) as hdul:\n",
    "        hdr = hdul[0].header\n",
    "        time_obs = hdr.get('DATE-OBS', 'UNKNOWN') \n",
    "    phot_table['time'] = time_obs\n",
    "\n",
    "    all_photometry.append(phot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9ec77-0adf-4688-a9f7-fc6c15edb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining results into a table.\n",
    "master_table = vstack(all_photometry)\n",
    "\n",
    "# Saving to a csv file. \n",
    "master_table.write('toi4463_photometry.csv', format='ascii.csv', overwrite=True)\n",
    "\n",
    "print(\"Aperture photometry complete for all 57 files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444f353-7d2e-424e-8b71-62cdfee95562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining png file.\n",
    "TOI4463_Transit_filename=\"toi4463_transit.png\"\n",
    "\n",
    "# Empty arrays for times, fluxes, and flux errors.\n",
    "times = []\n",
    "target_fluxes = []\n",
    "flux_errors = []\n",
    "\n",
    "\n",
    "for table in all_photometry:\n",
    "    filtered = table[table['aperture_radius'] == 9.32]\n",
    "    if len(filtered) < 6:\n",
    "        continue \n",
    "\n",
    "    time_str = filtered['time'][0]\n",
    "    if time_str == 'UNKNOWN':\n",
    "        continue\n",
    "\n",
    "    time_obj = Time(time_str, format='isot')\n",
    "    times.append(time_obj)\n",
    "\n",
    "    target_flux = filtered['net_flux'][0]\n",
    "    comp_fluxes = filtered['net_flux'][1:6]  # Using targets 1-5 for comparison\n",
    "    comp_flux = np.mean(comp_fluxes)\n",
    "\n",
    "    # Error propagation for photon noise. \n",
    "    err_target = np.sqrt(target_flux)\n",
    "    err_comps = np.sqrt(comp_fluxes)\n",
    "    err_comp = np.sqrt(np.sum(err_comps**2)) / 5.0  # Standard error of mean. \n",
    "\n",
    "    norm_flux = target_flux / comp_flux\n",
    "    norm_err = norm_flux * np.sqrt((err_target / target_flux)**2 + (err_comp / comp_flux)**2)\n",
    "    norm_err *= 75 # Exaggeration of error bars. \n",
    "\n",
    "    flux_errors.append(norm_err)\n",
    "    target_fluxes.append(norm_flux)\n",
    "\n",
    "# Sorting and unpacking data. \n",
    "sorted_data = sorted(zip(times, target_fluxes, flux_errors))\n",
    "times_sorted, flux_sorted, flux_err_sorted = zip(*sorted_data)\n",
    "times_sorted_utc = [t.to_datetime() for t in times_sorted]\n",
    "\n",
    "# Plotting transit curve with Normalized flux vs time in UTC.\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(times_sorted_utc, flux_sorted, yerr=flux_err_sorted, fmt='o', color='green', ecolor='gray', capsize=2)\n",
    "plt.xlabel(\"UTC Date\")\n",
    "plt.ylabel(\"Normalized Flux\")\n",
    "plt.title(\"Light Curve TOI 4463\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(TOI4463_Transit_filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48b69b-bfac-4f7f-ad27-c96ee47cff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining png for transit model curve\n",
    "TOI4463_Model_filename=\"toi4463_model.png\"\n",
    "\n",
    "# Convert your observation times (astropy.Time) to JD float\n",
    "times_jd = np.array([t.jd for t in times_sorted])  # times_sorted is your time list\n",
    "fluxes = np.array(flux_sorted)\n",
    "flux_errors = np.array(flux_err_sorted)\n",
    "\n",
    "# === Set up the batman transit model using actual JD times ===\n",
    "params = batman.TransitParams()\n",
    "params.t0 = 2460806.90                 # Transit mid-point in JD\n",
    "params.per = 6.54106                   # Period in days\n",
    "params.rp = 0.1183 / 1.056             # Radius of planet over stellar radius\n",
    "params.a = 15                          # Scaled semi-major axis (a/Rs)\n",
    "params.inc = 87                        # Inclination in degrees\n",
    "params.ecc = 0                         # Eccentricity\n",
    "params.w = 90                          # Longitude of periastron\n",
    "params.limb_dark = \"quadratic\"        # Limb darkening model\n",
    "params.u = [0.1, 0.3]                  # Limb darkening coefficients\n",
    "\n",
    "# Create model using JD times\n",
    "m = batman.TransitModel(params, times_jd)\n",
    "model_flux = m.light_curve(params)\n",
    "\n",
    "# Converting JD to UTC for x-axis plotting.\n",
    "times_utc = [Time(t, format='jd').to_datetime() for t in times_jd]\n",
    "\n",
    "# Plotting the flux vs Time in UTC\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(times_utc, fluxes, yerr=flux_errors, fmt='o', color='green', ecolor='gray', capsize=2, label='Observed Data')\n",
    "plt.plot(times_utc, model_flux, 'r-', label='Transit Model')\n",
    "plt.xlabel(\"UTC Time\")\n",
    "plt.ylabel(\"Normalized Flux\")\n",
    "plt.title(\"TOI 4463 Light Curve with Transit Model\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(TOI4463_Model_filename)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add6349-364c-44e5-9faa-1cbb351f1e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
